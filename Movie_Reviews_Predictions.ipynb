{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Sentiment of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this analysis is to predict if a review rates the movie positively or negatively. Inside this dataset there are 25,000 labeled movies reviews for training, 50,000 unlabeled reviews for training, and 25,000 reviews for testing. More information about the data can be found at: https://www.kaggle.com/c/word2vec-nlp-tutorial.\n",
    "\n",
    "This data comes from the 2015 Kaggle competition, \"Bag of Words Meets Bags of Popcorn.\" Despite the competition being finished, I thought it could still serve as a useful tool for my first Natural Lanugage Processing (NLP) project. Within this analysis you will find three methods for predicting the sentiment of movie reviews. I wanted to experiment with a few strategies to gain an understanding of different strategies and compare their results. The three methods that I will use are: \n",
    "- Bag of Centroids with Random Forest\n",
    "- Bag of Words with Tensorflow\n",
    "- Long Short Term Memory (LSTM) with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk.data\n",
    "import logging  \n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import time\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", \n",
    "                    header=0, \n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", \n",
    "                              header=0, \n",
    "                              delimiter=\"\\t\", \n",
    "                              quoting=3 )\n",
    "\n",
    "test = pd.read_csv(\"testData.tsv\", \n",
    "                   header=0, \n",
    "                   delimiter=\"\\t\", \n",
    "                   quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(50000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compare the lengths of the datasets\n",
    "print(train.shape)\n",
    "print(unlabeled_train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at a review\n",
    "train.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Naturally in a film who\\'s main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. However there is a craftsmanship and completeness to the film which anyone can enjoy. The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce\\'s short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good with the data. Naturally the reviews are of different lengths, but everything is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1: Bag of Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Clean the text, with the option to remove stopwords.\n",
    "\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    # Clean the text\n",
    "    review_text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'s\", \" \\'s\", review_text)\n",
    "    review_text = re.sub(r\"\\'ve\", \" \\'ve\", review_text)\n",
    "    review_text = re.sub(r\"n\\'t\", \" n\\'t\", review_text)\n",
    "    review_text = re.sub(r\"\\'re\", \" \\'re\", review_text)\n",
    "    review_text = re.sub(r\"\\'d\", \" \\'d\", review_text)\n",
    "    review_text = re.sub(r\"\\'ll\", \" \\'ll\", review_text)\n",
    "    review_text = re.sub(r\",\", \" , \", review_text)\n",
    "    review_text = re.sub(r\"!\", \" ! \", review_text)\n",
    "    review_text = re.sub(r\"\\(\", \" \\( \", review_text)\n",
    "    review_text = re.sub(r\"\\)\", \" \\) \", review_text)\n",
    "    review_text = re.sub(r\"\\?\", \" \\? \", review_text)\n",
    "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
    "\n",
    "    # Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    # Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # Shorten words to their stems (i.e. remove suffixes and other word endings)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Split a review into parsed sentences\n",
    "    # Returns a list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    # Use the NLTK tokenizer to split the review into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    # Return the list of sentences\n",
    "    # Each sentence is a list of words, so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = [] \n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print (\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "\n",
      "['with', 'all', 'this', 'stuff', 'go', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'start', 'listen', 'to', 'his', 'music', ',', 'watch', 'the', 'odd', 'documentari', 'here', 'and', 'there', ',', 'watch', 'the', 'wiz', 'and', 'watch', 'moonwalk', 'again']\n",
      "\n",
      "['mayb', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'realli', 'cool', 'in', 'the', 'eighti', 'just', 'to', 'mayb', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilti', 'or', 'innoc']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total \n",
    "print (len(sentences))\n",
    "print()\n",
    "print (sentences[0])\n",
    "print()\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 14:46:50,311 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-02-24 14:46:50,312 : INFO : collecting all words and their counts\n",
      "2017-02-24 14:46:50,313 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-24 14:46:50,361 : INFO : PROGRESS: at sentence #10000, processed 241944 words, keeping 12778 word types\n",
      "2017-02-24 14:46:50,406 : INFO : PROGRESS: at sentence #20000, processed 483715 words, keeping 17558 word types\n",
      "2017-02-24 14:46:50,450 : INFO : PROGRESS: at sentence #30000, processed 718901 words, keeping 20970 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 14:46:50,501 : INFO : PROGRESS: at sentence #40000, processed 961251 words, keeping 23861 word types\n",
      "2017-02-24 14:46:50,555 : INFO : PROGRESS: at sentence #50000, processed 1196241 words, keeping 26195 word types\n",
      "2017-02-24 14:46:50,601 : INFO : PROGRESS: at sentence #60000, processed 1433081 words, keeping 28180 word types\n",
      "2017-02-24 14:46:50,645 : INFO : PROGRESS: at sentence #70000, processed 1671693 words, keeping 30007 word types\n",
      "2017-02-24 14:46:50,689 : INFO : PROGRESS: at sentence #80000, processed 1906209 words, keeping 31644 word types\n",
      "2017-02-24 14:46:50,737 : INFO : PROGRESS: at sentence #90000, processed 2146479 words, keeping 33357 word types\n",
      "2017-02-24 14:46:50,783 : INFO : PROGRESS: at sentence #100000, processed 2384019 words, keeping 34803 word types\n",
      "2017-02-24 14:46:50,827 : INFO : PROGRESS: at sentence #110000, processed 2619451 words, keeping 36116 word types\n",
      "2017-02-24 14:46:50,873 : INFO : PROGRESS: at sentence #120000, processed 2856973 words, keeping 37554 word types\n",
      "2017-02-24 14:46:50,927 : INFO : PROGRESS: at sentence #130000, processed 3099090 words, keeping 38817 word types\n",
      "2017-02-24 14:46:50,969 : INFO : PROGRESS: at sentence #140000, processed 3326934 words, keeping 39897 word types\n",
      "2017-02-24 14:46:51,020 : INFO : PROGRESS: at sentence #150000, processed 3568173 words, keeping 41137 word types\n",
      "2017-02-24 14:46:51,072 : INFO : PROGRESS: at sentence #160000, processed 3806305 words, keeping 42280 word types\n",
      "2017-02-24 14:46:51,129 : INFO : PROGRESS: at sentence #170000, processed 4045131 words, keeping 43337 word types\n",
      "2017-02-24 14:46:51,179 : INFO : PROGRESS: at sentence #180000, processed 4280960 words, keeping 44379 word types\n",
      "2017-02-24 14:46:51,231 : INFO : PROGRESS: at sentence #190000, processed 4522012 words, keeping 45317 word types\n",
      "2017-02-24 14:46:51,281 : INFO : PROGRESS: at sentence #200000, processed 4762197 words, keeping 46208 word types\n",
      "2017-02-24 14:46:51,334 : INFO : PROGRESS: at sentence #210000, processed 4998984 words, keeping 47168 word types\n",
      "2017-02-24 14:46:51,381 : INFO : PROGRESS: at sentence #220000, processed 5239482 words, keeping 48150 word types\n",
      "2017-02-24 14:46:51,431 : INFO : PROGRESS: at sentence #230000, processed 5477559 words, keeping 49054 word types\n",
      "2017-02-24 14:46:51,483 : INFO : PROGRESS: at sentence #240000, processed 5720895 words, keeping 49980 word types\n",
      "2017-02-24 14:46:51,529 : INFO : PROGRESS: at sentence #250000, processed 5950126 words, keeping 50875 word types\n",
      "2017-02-24 14:46:51,576 : INFO : PROGRESS: at sentence #260000, processed 6186122 words, keeping 51696 word types\n",
      "2017-02-24 14:46:51,624 : INFO : PROGRESS: at sentence #270000, processed 6422932 words, keeping 52708 word types\n",
      "2017-02-24 14:46:51,678 : INFO : PROGRESS: at sentence #280000, processed 6664298 words, keeping 53971 word types\n",
      "2017-02-24 14:46:51,728 : INFO : PROGRESS: at sentence #290000, processed 6903069 words, keeping 55131 word types\n",
      "2017-02-24 14:46:51,778 : INFO : PROGRESS: at sentence #300000, processed 7143798 words, keeping 56200 word types\n",
      "2017-02-24 14:46:51,833 : INFO : PROGRESS: at sentence #310000, processed 7384927 words, keeping 57241 word types\n",
      "2017-02-24 14:46:51,881 : INFO : PROGRESS: at sentence #320000, processed 7625288 words, keeping 58341 word types\n",
      "2017-02-24 14:46:51,932 : INFO : PROGRESS: at sentence #330000, processed 7862258 words, keeping 59327 word types\n",
      "2017-02-24 14:46:51,986 : INFO : PROGRESS: at sentence #340000, processed 8107924 words, keeping 60316 word types\n",
      "2017-02-24 14:46:52,035 : INFO : PROGRESS: at sentence #350000, processed 8346502 words, keeping 61223 word types\n",
      "2017-02-24 14:46:52,085 : INFO : PROGRESS: at sentence #360000, processed 8582572 words, keeping 62133 word types\n",
      "2017-02-24 14:46:52,134 : INFO : PROGRESS: at sentence #370000, processed 8825626 words, keeping 62989 word types\n",
      "2017-02-24 14:46:52,184 : INFO : PROGRESS: at sentence #380000, processed 9066576 words, keeping 63940 word types\n",
      "2017-02-24 14:46:52,236 : INFO : PROGRESS: at sentence #390000, processed 9312667 words, keeping 64751 word types\n",
      "2017-02-24 14:46:52,282 : INFO : PROGRESS: at sentence #400000, processed 9551286 words, keeping 65541 word types\n",
      "2017-02-24 14:46:52,330 : INFO : PROGRESS: at sentence #410000, processed 9788207 words, keeping 66297 word types\n",
      "2017-02-24 14:46:52,380 : INFO : PROGRESS: at sentence #420000, processed 10024848 words, keeping 67100 word types\n",
      "2017-02-24 14:46:52,434 : INFO : PROGRESS: at sentence #430000, processed 10268539 words, keeping 67908 word types\n",
      "2017-02-24 14:46:52,490 : INFO : PROGRESS: at sentence #440000, processed 10511346 words, keeping 68704 word types\n",
      "2017-02-24 14:46:52,550 : INFO : PROGRESS: at sentence #450000, processed 10750684 words, keeping 69596 word types\n",
      "2017-02-24 14:46:52,602 : INFO : PROGRESS: at sentence #460000, processed 10999696 words, keeping 70416 word types\n",
      "2017-02-24 14:46:52,662 : INFO : PROGRESS: at sentence #470000, processed 11243575 words, keeping 71086 word types\n",
      "2017-02-24 14:46:52,713 : INFO : PROGRESS: at sentence #480000, processed 11479656 words, keeping 71869 word types\n",
      "2017-02-24 14:46:52,766 : INFO : PROGRESS: at sentence #490000, processed 11722966 words, keeping 72729 word types\n",
      "2017-02-24 14:46:52,817 : INFO : PROGRESS: at sentence #500000, processed 11960309 words, keeping 73408 word types\n",
      "2017-02-24 14:46:52,867 : INFO : PROGRESS: at sentence #510000, processed 12201067 words, keeping 74156 word types\n",
      "2017-02-24 14:46:52,916 : INFO : PROGRESS: at sentence #520000, processed 12439928 words, keeping 74875 word types\n",
      "2017-02-24 14:46:52,965 : INFO : PROGRESS: at sentence #530000, processed 12680222 words, keeping 75536 word types\n",
      "2017-02-24 14:46:53,011 : INFO : PROGRESS: at sentence #540000, processed 12920376 words, keeping 76251 word types\n",
      "2017-02-24 14:46:53,062 : INFO : PROGRESS: at sentence #550000, processed 13161975 words, keeping 76962 word types\n",
      "2017-02-24 14:46:53,111 : INFO : PROGRESS: at sentence #560000, processed 13398809 words, keeping 77645 word types\n",
      "2017-02-24 14:46:53,170 : INFO : PROGRESS: at sentence #570000, processed 13643865 words, keeping 78248 word types\n",
      "2017-02-24 14:46:53,222 : INFO : PROGRESS: at sentence #580000, processed 13881149 words, keeping 78937 word types\n",
      "2017-02-24 14:46:53,279 : INFO : PROGRESS: at sentence #590000, processed 14122677 words, keeping 79632 word types\n",
      "2017-02-24 14:46:53,327 : INFO : PROGRESS: at sentence #600000, processed 14360415 words, keeping 80241 word types\n",
      "2017-02-24 14:46:53,377 : INFO : PROGRESS: at sentence #610000, processed 14597385 words, keeping 80967 word types\n",
      "2017-02-24 14:46:53,424 : INFO : PROGRESS: at sentence #620000, processed 14839693 words, keeping 81568 word types\n",
      "2017-02-24 14:46:53,475 : INFO : PROGRESS: at sentence #630000, processed 15079493 words, keeping 82177 word types\n",
      "2017-02-24 14:46:53,522 : INFO : PROGRESS: at sentence #640000, processed 15315304 words, keeping 82844 word types\n",
      "2017-02-24 14:46:53,570 : INFO : PROGRESS: at sentence #650000, processed 15557008 words, keeping 83498 word types\n",
      "2017-02-24 14:46:53,619 : INFO : PROGRESS: at sentence #660000, processed 15795701 words, keeping 84117 word types\n",
      "2017-02-24 14:46:53,667 : INFO : PROGRESS: at sentence #670000, processed 16034319 words, keeping 84671 word types\n",
      "2017-02-24 14:46:53,720 : INFO : PROGRESS: at sentence #680000, processed 16275496 words, keeping 85240 word types\n",
      "2017-02-24 14:46:53,769 : INFO : PROGRESS: at sentence #690000, processed 16513557 words, keeping 85853 word types\n",
      "2017-02-24 14:46:53,822 : INFO : PROGRESS: at sentence #700000, processed 16758022 words, keeping 86481 word types\n",
      "2017-02-24 14:46:53,871 : INFO : PROGRESS: at sentence #710000, processed 16996835 words, keeping 86989 word types\n",
      "2017-02-24 14:46:53,928 : INFO : PROGRESS: at sentence #720000, processed 17237798 words, keeping 87494 word types\n",
      "2017-02-24 14:46:53,978 : INFO : PROGRESS: at sentence #730000, processed 17479884 words, keeping 88066 word types\n",
      "2017-02-24 14:46:54,025 : INFO : PROGRESS: at sentence #740000, processed 17716197 words, keeping 88647 word types\n",
      "2017-02-24 14:46:54,079 : INFO : PROGRESS: at sentence #750000, processed 17950020 words, keeping 89157 word types\n",
      "2017-02-24 14:46:54,129 : INFO : PROGRESS: at sentence #760000, processed 18185057 words, keeping 89674 word types\n",
      "2017-02-24 14:46:54,180 : INFO : PROGRESS: at sentence #770000, processed 18428411 words, keeping 90286 word types\n",
      "2017-02-24 14:46:54,232 : INFO : PROGRESS: at sentence #780000, processed 18674595 words, keeping 90842 word types\n",
      "2017-02-24 14:46:54,284 : INFO : PROGRESS: at sentence #790000, processed 18917675 words, keeping 91382 word types\n",
      "2017-02-24 14:46:54,313 : INFO : collected 91732 word types from a corpus of 19049517 raw words and 795538 sentences\n",
      "2017-02-24 14:46:54,314 : INFO : Loading a fresh vocabulary\n",
      "2017-02-24 14:46:54,413 : INFO : min_count=20 retains 17218 unique words (18% of original 91732, drops 74514)\n",
      "2017-02-24 14:46:54,414 : INFO : min_count=20 leaves 18813127 word corpus (98% of original 19049517, drops 236390)\n",
      "2017-02-24 14:46:54,499 : INFO : deleting the raw counts dictionary of 91732 items\n",
      "2017-02-24 14:46:54,505 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-02-24 14:46:54,506 : INFO : downsampling leaves estimated 13619751 word corpus (72.4% of prior 18813127)\n",
      "2017-02-24 14:46:54,506 : INFO : estimated required memory for 17218 words and 250 dimensions: 43045000 bytes\n",
      "2017-02-24 14:46:54,579 : INFO : resetting layer weights\n",
      "2017-02-24 14:46:54,957 : INFO : training model with 1 workers on 17218 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=20\n",
      "2017-02-24 14:46:54,958 : INFO : expecting 795538 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-24 14:46:55,965 : INFO : PROGRESS: at 0.31% examples, 213595 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:46:56,970 : INFO : PROGRESS: at 0.62% examples, 209815 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:46:57,974 : INFO : PROGRESS: at 0.93% examples, 210895 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:46:58,995 : INFO : PROGRESS: at 1.26% examples, 212230 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:46:59,999 : INFO : PROGRESS: at 1.59% examples, 213783 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:01,024 : INFO : PROGRESS: at 1.91% examples, 214218 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:02,057 : INFO : PROGRESS: at 2.25% examples, 215198 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:03,080 : INFO : PROGRESS: at 2.58% examples, 215385 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:04,104 : INFO : PROGRESS: at 2.91% examples, 215439 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:05,120 : INFO : PROGRESS: at 3.23% examples, 215709 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:06,146 : INFO : PROGRESS: at 3.57% examples, 215671 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:07,158 : INFO : PROGRESS: at 3.89% examples, 215889 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:08,185 : INFO : PROGRESS: at 4.21% examples, 215300 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:09,200 : INFO : PROGRESS: at 4.54% examples, 215502 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:10,226 : INFO : PROGRESS: at 4.86% examples, 215537 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:11,228 : INFO : PROGRESS: at 5.19% examples, 215815 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:12,247 : INFO : PROGRESS: at 5.51% examples, 215875 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:13,263 : INFO : PROGRESS: at 5.83% examples, 215992 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:14,275 : INFO : PROGRESS: at 6.16% examples, 216127 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:15,301 : INFO : PROGRESS: at 6.49% examples, 216095 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:16,312 : INFO : PROGRESS: at 6.82% examples, 216222 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:17,336 : INFO : PROGRESS: at 7.15% examples, 216224 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:18,350 : INFO : PROGRESS: at 7.47% examples, 216303 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:19,369 : INFO : PROGRESS: at 7.79% examples, 216331 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:20,388 : INFO : PROGRESS: at 8.12% examples, 216359 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:21,411 : INFO : PROGRESS: at 8.44% examples, 216371 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:22,428 : INFO : PROGRESS: at 8.76% examples, 216420 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:23,445 : INFO : PROGRESS: at 9.09% examples, 216452 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:24,468 : INFO : PROGRESS: at 9.41% examples, 216437 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:25,479 : INFO : PROGRESS: at 9.73% examples, 216509 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:26,496 : INFO : PROGRESS: at 10.05% examples, 216545 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:27,527 : INFO : PROGRESS: at 10.39% examples, 216708 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:28,542 : INFO : PROGRESS: at 10.72% examples, 216740 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:29,571 : INFO : PROGRESS: at 11.04% examples, 216685 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:30,578 : INFO : PROGRESS: at 11.36% examples, 216783 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:31,597 : INFO : PROGRESS: at 11.68% examples, 216776 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:32,615 : INFO : PROGRESS: at 12.00% examples, 216789 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:33,625 : INFO : PROGRESS: at 12.32% examples, 216856 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:34,650 : INFO : PROGRESS: at 12.65% examples, 216827 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:35,658 : INFO : PROGRESS: at 12.97% examples, 216894 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:36,684 : INFO : PROGRESS: at 13.30% examples, 217039 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:37,694 : INFO : PROGRESS: at 13.63% examples, 217086 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:38,718 : INFO : PROGRESS: at 13.95% examples, 217063 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:39,721 : INFO : PROGRESS: at 14.27% examples, 216986 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:40,744 : INFO : PROGRESS: at 14.59% examples, 216978 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:41,774 : INFO : PROGRESS: at 14.93% examples, 217076 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:42,783 : INFO : PROGRESS: at 15.26% examples, 217123 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:43,805 : INFO : PROGRESS: at 15.58% examples, 217110 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:44,833 : INFO : PROGRESS: at 15.90% examples, 217072 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:45,841 : INFO : PROGRESS: at 16.23% examples, 217113 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:46,846 : INFO : PROGRESS: at 16.55% examples, 217169 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:47,865 : INFO : PROGRESS: at 16.88% examples, 217162 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:48,896 : INFO : PROGRESS: at 17.20% examples, 217111 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:49,927 : INFO : PROGRESS: at 17.54% examples, 217187 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:50,950 : INFO : PROGRESS: at 17.86% examples, 217166 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:51,964 : INFO : PROGRESS: at 18.18% examples, 217184 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:52,977 : INFO : PROGRESS: at 18.51% examples, 217214 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:53,997 : INFO : PROGRESS: at 18.84% examples, 217211 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:55,010 : INFO : PROGRESS: at 19.17% examples, 217227 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:56,047 : INFO : PROGRESS: at 19.49% examples, 217159 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:47:57,064 : INFO : PROGRESS: at 19.80% examples, 217170 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:58,089 : INFO : PROGRESS: at 20.13% examples, 217147 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:47:59,103 : INFO : PROGRESS: at 20.45% examples, 217165 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:00,111 : INFO : PROGRESS: at 20.78% examples, 217195 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:01,138 : INFO : PROGRESS: at 21.10% examples, 217165 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:02,161 : INFO : PROGRESS: at 21.43% examples, 217149 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:03,181 : INFO : PROGRESS: at 21.76% examples, 217143 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:04,198 : INFO : PROGRESS: at 22.08% examples, 217155 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:05,218 : INFO : PROGRESS: at 22.41% examples, 217152 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:06,234 : INFO : PROGRESS: at 22.73% examples, 217060 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:07,251 : INFO : PROGRESS: at 23.06% examples, 217066 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:08,268 : INFO : PROGRESS: at 23.39% examples, 217068 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:09,291 : INFO : PROGRESS: at 23.71% examples, 217053 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:10,292 : INFO : PROGRESS: at 24.03% examples, 217009 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:11,305 : INFO : PROGRESS: at 24.36% examples, 217029 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:12,321 : INFO : PROGRESS: at 24.68% examples, 217044 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:13,343 : INFO : PROGRESS: at 25.01% examples, 217031 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:14,365 : INFO : PROGRESS: at 25.33% examples, 217028 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:15,375 : INFO : PROGRESS: at 25.66% examples, 217054 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:16,386 : INFO : PROGRESS: at 25.98% examples, 217075 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:17,391 : INFO : PROGRESS: at 26.31% examples, 217022 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:18,404 : INFO : PROGRESS: at 26.64% examples, 217031 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:19,417 : INFO : PROGRESS: at 26.96% examples, 217045 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:20,430 : INFO : PROGRESS: at 27.29% examples, 217062 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:21,433 : INFO : PROGRESS: at 27.61% examples, 217097 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:22,450 : INFO : PROGRESS: at 27.93% examples, 217105 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:23,477 : INFO : PROGRESS: at 28.26% examples, 217084 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:24,477 : INFO : PROGRESS: at 28.57% examples, 217052 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:25,494 : INFO : PROGRESS: at 28.89% examples, 217055 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:26,499 : INFO : PROGRESS: at 29.22% examples, 217089 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:27,528 : INFO : PROGRESS: at 29.54% examples, 217064 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:28,538 : INFO : PROGRESS: at 29.86% examples, 217088 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:29,547 : INFO : PROGRESS: at 30.18% examples, 217113 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:30,554 : INFO : PROGRESS: at 30.51% examples, 217139 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:31,567 : INFO : PROGRESS: at 30.83% examples, 217150 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:32,587 : INFO : PROGRESS: at 31.16% examples, 217148 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:33,600 : INFO : PROGRESS: at 31.47% examples, 217159 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:34,603 : INFO : PROGRESS: at 31.78% examples, 217123 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:35,626 : INFO : PROGRESS: at 32.11% examples, 217110 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:36,664 : INFO : PROGRESS: at 32.43% examples, 217072 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:37,694 : INFO : PROGRESS: at 32.76% examples, 217051 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:38,706 : INFO : PROGRESS: at 33.08% examples, 217068 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:39,724 : INFO : PROGRESS: at 33.40% examples, 217067 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:40,752 : INFO : PROGRESS: at 33.74% examples, 217112 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:41,776 : INFO : PROGRESS: at 34.06% examples, 217102 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:42,778 : INFO : PROGRESS: at 34.37% examples, 217071 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:43,780 : INFO : PROGRESS: at 34.69% examples, 217038 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:44,789 : INFO : PROGRESS: at 35.01% examples, 217062 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:45,794 : INFO : PROGRESS: at 35.34% examples, 217087 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:46,796 : INFO : PROGRESS: at 35.66% examples, 217119 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:47,817 : INFO : PROGRESS: at 35.99% examples, 217115 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:48,831 : INFO : PROGRESS: at 36.31% examples, 217125 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:49,840 : INFO : PROGRESS: at 36.63% examples, 217079 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:50,860 : INFO : PROGRESS: at 36.95% examples, 217079 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:51,884 : INFO : PROGRESS: at 37.28% examples, 217070 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:52,891 : INFO : PROGRESS: at 37.59% examples, 217035 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:53,896 : INFO : PROGRESS: at 37.91% examples, 217053 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:54,923 : INFO : PROGRESS: at 38.23% examples, 217037 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:55,942 : INFO : PROGRESS: at 38.56% examples, 217043 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:48:56,949 : INFO : PROGRESS: at 38.89% examples, 217063 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:57,967 : INFO : PROGRESS: at 39.22% examples, 217065 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:58,977 : INFO : PROGRESS: at 39.54% examples, 217078 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:48:59,977 : INFO : PROGRESS: at 39.85% examples, 217056 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:00,985 : INFO : PROGRESS: at 40.17% examples, 217073 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:02,009 : INFO : PROGRESS: at 40.49% examples, 217067 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:03,036 : INFO : PROGRESS: at 40.83% examples, 217110 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:04,058 : INFO : PROGRESS: at 41.15% examples, 217100 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:05,085 : INFO : PROGRESS: at 41.48% examples, 217088 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:06,102 : INFO : PROGRESS: at 41.81% examples, 217092 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:07,114 : INFO : PROGRESS: at 42.14% examples, 217106 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:08,147 : INFO : PROGRESS: at 42.47% examples, 217085 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:09,171 : INFO : PROGRESS: at 42.80% examples, 217077 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:10,185 : INFO : PROGRESS: at 43.12% examples, 217083 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:11,222 : INFO : PROGRESS: at 43.47% examples, 217108 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:12,247 : INFO : PROGRESS: at 43.79% examples, 217095 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:13,250 : INFO : PROGRESS: at 44.12% examples, 217120 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:14,256 : INFO : PROGRESS: at 44.45% examples, 217142 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:15,290 : INFO : PROGRESS: at 44.78% examples, 217173 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:16,292 : INFO : PROGRESS: at 45.11% examples, 217193 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:17,311 : INFO : PROGRESS: at 45.43% examples, 217194 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:18,321 : INFO : PROGRESS: at 45.76% examples, 217206 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:19,330 : INFO : PROGRESS: at 46.08% examples, 217221 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:20,336 : INFO : PROGRESS: at 46.42% examples, 217234 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:21,351 : INFO : PROGRESS: at 46.74% examples, 217237 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:22,372 : INFO : PROGRESS: at 47.07% examples, 217235 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:23,401 : INFO : PROGRESS: at 47.39% examples, 217223 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:24,410 : INFO : PROGRESS: at 47.71% examples, 217234 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:25,413 : INFO : PROGRESS: at 48.04% examples, 217255 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:26,424 : INFO : PROGRESS: at 48.36% examples, 217265 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:27,454 : INFO : PROGRESS: at 48.68% examples, 217249 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:28,472 : INFO : PROGRESS: at 49.01% examples, 217251 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:29,479 : INFO : PROGRESS: at 49.33% examples, 217266 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:30,490 : INFO : PROGRESS: at 49.65% examples, 217274 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:31,499 : INFO : PROGRESS: at 49.97% examples, 217285 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:32,527 : INFO : PROGRESS: at 50.30% examples, 217274 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:33,552 : INFO : PROGRESS: at 50.63% examples, 217265 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:34,565 : INFO : PROGRESS: at 50.95% examples, 217272 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:35,579 : INFO : PROGRESS: at 51.26% examples, 217233 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:36,609 : INFO : PROGRESS: at 51.59% examples, 217260 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:37,627 : INFO : PROGRESS: at 51.91% examples, 217260 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:38,646 : INFO : PROGRESS: at 52.23% examples, 217255 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:39,665 : INFO : PROGRESS: at 52.56% examples, 217254 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:40,689 : INFO : PROGRESS: at 52.88% examples, 217248 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:41,719 : INFO : PROGRESS: at 53.21% examples, 217232 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:42,733 : INFO : PROGRESS: at 53.53% examples, 217240 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:43,749 : INFO : PROGRESS: at 53.85% examples, 217244 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:44,756 : INFO : PROGRESS: at 54.17% examples, 217221 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:45,781 : INFO : PROGRESS: at 54.49% examples, 217209 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:46,788 : INFO : PROGRESS: at 54.82% examples, 217226 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:47,815 : INFO : PROGRESS: at 55.14% examples, 217216 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:48,832 : INFO : PROGRESS: at 55.46% examples, 217219 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:49,842 : INFO : PROGRESS: at 55.79% examples, 217229 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:50,860 : INFO : PROGRESS: at 56.12% examples, 217230 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:51,869 : INFO : PROGRESS: at 56.44% examples, 217242 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:52,892 : INFO : PROGRESS: at 56.77% examples, 217237 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:53,914 : INFO : PROGRESS: at 57.09% examples, 217234 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:54,914 : INFO : PROGRESS: at 57.40% examples, 217214 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:55,920 : INFO : PROGRESS: at 57.73% examples, 217229 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:49:56,946 : INFO : PROGRESS: at 58.05% examples, 217219 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:57,964 : INFO : PROGRESS: at 58.37% examples, 217220 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:49:58,978 : INFO : PROGRESS: at 58.70% examples, 217226 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:00,001 : INFO : PROGRESS: at 59.03% examples, 217222 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:01,021 : INFO : PROGRESS: at 59.36% examples, 217218 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:02,023 : INFO : PROGRESS: at 59.67% examples, 217199 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:03,038 : INFO : PROGRESS: at 59.99% examples, 217205 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:04,046 : INFO : PROGRESS: at 60.31% examples, 217216 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:05,069 : INFO : PROGRESS: at 60.63% examples, 217209 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:06,082 : INFO : PROGRESS: at 60.96% examples, 217215 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:07,097 : INFO : PROGRESS: at 61.28% examples, 217215 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:08,108 : INFO : PROGRESS: at 61.62% examples, 217222 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:09,118 : INFO : PROGRESS: at 61.94% examples, 217233 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:10,121 : INFO : PROGRESS: at 62.26% examples, 217213 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:11,138 : INFO : PROGRESS: at 62.59% examples, 217215 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:12,139 : INFO : PROGRESS: at 62.91% examples, 217199 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:13,150 : INFO : PROGRESS: at 63.23% examples, 217210 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:14,164 : INFO : PROGRESS: at 63.56% examples, 217213 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:15,168 : INFO : PROGRESS: at 63.89% examples, 217227 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:16,195 : INFO : PROGRESS: at 64.23% examples, 217249 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:17,205 : INFO : PROGRESS: at 64.55% examples, 217258 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:18,220 : INFO : PROGRESS: at 64.88% examples, 217261 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:19,240 : INFO : PROGRESS: at 65.20% examples, 217258 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:20,256 : INFO : PROGRESS: at 65.53% examples, 217259 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:21,290 : INFO : PROGRESS: at 65.85% examples, 217243 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:22,295 : INFO : PROGRESS: at 66.18% examples, 217257 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:23,306 : INFO : PROGRESS: at 66.51% examples, 217262 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:24,315 : INFO : PROGRESS: at 66.84% examples, 217271 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:25,331 : INFO : PROGRESS: at 67.16% examples, 217273 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:26,355 : INFO : PROGRESS: at 67.49% examples, 217266 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:27,381 : INFO : PROGRESS: at 67.81% examples, 217256 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:28,393 : INFO : PROGRESS: at 68.13% examples, 217262 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:29,412 : INFO : PROGRESS: at 68.46% examples, 217261 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:30,433 : INFO : PROGRESS: at 68.78% examples, 217259 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:31,451 : INFO : PROGRESS: at 69.11% examples, 217258 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:32,455 : INFO : PROGRESS: at 69.41% examples, 217239 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:33,480 : INFO : PROGRESS: at 69.74% examples, 217231 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:34,496 : INFO : PROGRESS: at 70.06% examples, 217235 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:35,502 : INFO : PROGRESS: at 70.39% examples, 217247 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:36,521 : INFO : PROGRESS: at 70.72% examples, 217246 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:37,534 : INFO : PROGRESS: at 71.04% examples, 217249 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:38,540 : INFO : PROGRESS: at 71.36% examples, 217264 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:39,555 : INFO : PROGRESS: at 71.66% examples, 217234 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:40,581 : INFO : PROGRESS: at 71.98% examples, 217225 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:41,611 : INFO : PROGRESS: at 72.31% examples, 217214 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:42,614 : INFO : PROGRESS: at 72.62% examples, 217198 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:43,621 : INFO : PROGRESS: at 72.94% examples, 217179 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:44,640 : INFO : PROGRESS: at 73.26% examples, 217179 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:45,644 : INFO : PROGRESS: at 73.59% examples, 217192 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:46,645 : INFO : PROGRESS: at 73.90% examples, 217179 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:47,670 : INFO : PROGRESS: at 74.22% examples, 217173 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:48,691 : INFO : PROGRESS: at 74.55% examples, 217173 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:49,711 : INFO : PROGRESS: at 74.87% examples, 217172 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:50,737 : INFO : PROGRESS: at 75.21% examples, 217194 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:51,742 : INFO : PROGRESS: at 75.52% examples, 217175 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:52,748 : INFO : PROGRESS: at 75.84% examples, 217158 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:53,760 : INFO : PROGRESS: at 76.16% examples, 217163 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:54,774 : INFO : PROGRESS: at 76.49% examples, 217168 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:55,791 : INFO : PROGRESS: at 76.81% examples, 217169 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:56,819 : INFO : PROGRESS: at 77.14% examples, 217162 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:57,836 : INFO : PROGRESS: at 77.46% examples, 217162 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:50:58,844 : INFO : PROGRESS: at 77.78% examples, 217171 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:50:59,870 : INFO : PROGRESS: at 78.11% examples, 217165 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:00,900 : INFO : PROGRESS: at 78.43% examples, 217156 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:01,915 : INFO : PROGRESS: at 78.76% examples, 217159 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:02,923 : INFO : PROGRESS: at 79.09% examples, 217170 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:03,941 : INFO : PROGRESS: at 79.41% examples, 217168 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:04,958 : INFO : PROGRESS: at 79.73% examples, 217172 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:05,977 : INFO : PROGRESS: at 80.05% examples, 217171 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:06,983 : INFO : PROGRESS: at 80.37% examples, 217181 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:07,999 : INFO : PROGRESS: at 80.70% examples, 217185 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:09,005 : INFO : PROGRESS: at 81.02% examples, 217195 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:10,017 : INFO : PROGRESS: at 81.35% examples, 217201 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:11,033 : INFO : PROGRESS: at 81.68% examples, 217204 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:12,057 : INFO : PROGRESS: at 82.01% examples, 217200 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:13,064 : INFO : PROGRESS: at 82.32% examples, 217182 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:14,073 : INFO : PROGRESS: at 82.65% examples, 217191 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:15,090 : INFO : PROGRESS: at 82.98% examples, 217193 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:16,104 : INFO : PROGRESS: at 83.31% examples, 217196 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:17,114 : INFO : PROGRESS: at 83.64% examples, 217202 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:18,141 : INFO : PROGRESS: at 83.96% examples, 217195 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:19,164 : INFO : PROGRESS: at 84.29% examples, 217192 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:20,181 : INFO : PROGRESS: at 84.62% examples, 217193 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:21,191 : INFO : PROGRESS: at 84.94% examples, 217201 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:22,198 : INFO : PROGRESS: at 85.27% examples, 217211 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:23,207 : INFO : PROGRESS: at 85.59% examples, 217219 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:24,223 : INFO : PROGRESS: at 85.91% examples, 217220 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:25,234 : INFO : PROGRESS: at 86.25% examples, 217226 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:26,279 : INFO : PROGRESS: at 86.51% examples, 217045 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:27,479 : INFO : PROGRESS: at 86.63% examples, 216378 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:28,498 : INFO : PROGRESS: at 86.83% examples, 216066 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:29,513 : INFO : PROGRESS: at 87.06% examples, 215839 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:30,525 : INFO : PROGRESS: at 87.35% examples, 215746 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:31,554 : INFO : PROGRESS: at 87.60% examples, 215587 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:32,584 : INFO : PROGRESS: at 87.86% examples, 215403 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:33,603 : INFO : PROGRESS: at 88.15% examples, 215331 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:34,622 : INFO : PROGRESS: at 88.40% examples, 215159 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:35,648 : INFO : PROGRESS: at 88.58% examples, 214831 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:36,659 : INFO : PROGRESS: at 88.78% examples, 214542 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:37,671 : INFO : PROGRESS: at 88.85% examples, 213924 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:38,709 : INFO : PROGRESS: at 89.09% examples, 213720 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:39,723 : INFO : PROGRESS: at 89.32% examples, 213509 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:40,779 : INFO : PROGRESS: at 89.54% examples, 213243 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:41,791 : INFO : PROGRESS: at 89.79% examples, 213113 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:42,794 : INFO : PROGRESS: at 90.10% examples, 213089 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:43,803 : INFO : PROGRESS: at 90.39% examples, 213037 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:44,816 : INFO : PROGRESS: at 90.67% examples, 212957 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:45,824 : INFO : PROGRESS: at 90.93% examples, 212831 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:46,856 : INFO : PROGRESS: at 91.23% examples, 212764 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:48,093 : INFO : PROGRESS: at 91.40% examples, 212280 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:49,113 : INFO : PROGRESS: at 91.45% examples, 211666 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:50,141 : INFO : PROGRESS: at 91.66% examples, 211434 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:51,154 : INFO : PROGRESS: at 91.94% examples, 211362 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:52,164 : INFO : PROGRESS: at 92.22% examples, 211267 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:53,187 : INFO : PROGRESS: at 92.49% examples, 211166 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:51:54,216 : INFO : PROGRESS: at 92.70% examples, 210916 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:55,225 : INFO : PROGRESS: at 92.89% examples, 210636 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:56,240 : INFO : PROGRESS: at 93.18% examples, 210590 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:57,313 : INFO : PROGRESS: at 93.42% examples, 210386 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:58,336 : INFO : PROGRESS: at 93.67% examples, 210241 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:51:59,486 : INFO : PROGRESS: at 93.79% examples, 209704 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:00,493 : INFO : PROGRESS: at 93.92% examples, 209315 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:01,525 : INFO : PROGRESS: at 94.21% examples, 209240 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:02,549 : INFO : PROGRESS: at 94.52% examples, 209238 words/s, in_qsize 2, out_qsize 0\n",
      "2017-02-24 14:52:03,575 : INFO : PROGRESS: at 94.82% examples, 209215 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:04,600 : INFO : PROGRESS: at 95.12% examples, 209192 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:05,609 : INFO : PROGRESS: at 95.37% examples, 209041 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:06,614 : INFO : PROGRESS: at 95.63% examples, 208940 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:07,662 : INFO : PROGRESS: at 95.85% examples, 208720 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:08,742 : INFO : PROGRESS: at 96.09% examples, 208523 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:09,787 : INFO : PROGRESS: at 96.14% examples, 207944 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:10,802 : INFO : PROGRESS: at 96.32% examples, 207659 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:11,828 : INFO : PROGRESS: at 96.57% examples, 207528 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:12,845 : INFO : PROGRESS: at 96.88% examples, 207516 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:13,853 : INFO : PROGRESS: at 97.15% examples, 207443 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:14,859 : INFO : PROGRESS: at 97.43% examples, 207390 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:15,865 : INFO : PROGRESS: at 97.71% examples, 207342 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:16,878 : INFO : PROGRESS: at 98.00% examples, 207309 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:17,903 : INFO : PROGRESS: at 98.31% examples, 207292 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:18,930 : INFO : PROGRESS: at 98.62% examples, 207295 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:19,936 : INFO : PROGRESS: at 98.90% examples, 207224 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:21,000 : INFO : PROGRESS: at 98.95% examples, 206657 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:22,025 : INFO : PROGRESS: at 99.14% examples, 206381 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:23,055 : INFO : PROGRESS: at 99.38% examples, 206255 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:24,061 : INFO : PROGRESS: at 99.67% examples, 206232 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:25,093 : INFO : PROGRESS: at 99.97% examples, 206213 words/s, in_qsize 1, out_qsize 0\n",
      "2017-02-24 14:52:25,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-24 14:52:25,189 : INFO : training on 95247585 raw words (68099821 effective words) took 330.2s, 206216 effective words/s\n",
      "2017-02-24 14:52:25,189 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-02-24 14:52:25,392 : INFO : saving Word2Vec object under 250features_20minwords_20context, separately None\n",
      "2017-02-24 14:52:25,393 : INFO : not storing attribute cum_table\n",
      "2017-02-24 14:52:25,393 : INFO : not storing attribute syn0norm\n",
      "2017-02-24 14:52:26,936 : INFO : saved 250features_20minwords_20context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 250      # Word vector dimensionality                      \n",
    "min_word_count = 20     # Minimum word count                        \n",
    "num_workers = 1         # Number of threads to run in parallel\n",
    "context = 20            # Context window size                                                                                    \n",
    "downsampling = 1e-3     # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context, \n",
    "                          sample = downsampling)\n",
    "\n",
    "# Call init_sims because we won't train the model any further \n",
    "# This will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model for potential, future use.\n",
    "model_name = \"250features_20minwords_20context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the model, if necessary\n",
    "# model = Word2Vec.load(\"250features_20minwords_20context\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.613450825214386), ('businessman', 0.5231447219848633), ('loner', 0.49102887511253357), ('ladi', 0.4824357330799103), ('lad', 0.4822234809398651), ('millionair', 0.4815925359725952), ('boxer', 0.48143839836120605), ('men', 0.465503454208374), ('crippl', 0.4626684784889221), ('policeman', 0.442837119102478)]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the performance of the model\n",
    "print(model.most_similar(\"man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantast', 0.7024080753326416),\n",
       " ('terrif', 0.7019121050834656),\n",
       " ('superb', 0.6130414009094238),\n",
       " ('fine', 0.5928903222084045),\n",
       " ('excel', 0.5899552702903748),\n",
       " ('brilliant', 0.5725492835044861),\n",
       " ('good', 0.5600504875183105),\n",
       " ('solid', 0.5397793054580688),\n",
       " ('tremend', 0.5350014567375183),\n",
       " ('fabul', 0.5297815203666687)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horribl', 0.8503983616828918),\n",
       " ('horrend', 0.7642723321914673),\n",
       " ('atroci', 0.7585293650627136),\n",
       " ('aw', 0.7438712120056152),\n",
       " ('horrid', 0.7418596744537354),\n",
       " ('lousi', 0.6711216568946838),\n",
       " ('abysm', 0.6696984171867371),\n",
       " ('shoddi', 0.6531139016151428),\n",
       " ('bad', 0.6492224931716919),\n",
       " ('laughabl', 0.633672297000885)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"terribl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.8010729551315308),\n",
       " ('flick', 0.642029881477356),\n",
       " ('it', 0.5277636647224426),\n",
       " ('sequel', 0.45611339807510376),\n",
       " ('thing', 0.45259204506874084),\n",
       " ('storylin', 0.4260961711406708),\n",
       " ('stinker', 0.4210663139820099),\n",
       " ('documentari', 0.40751001238822937),\n",
       " ('cinema', 0.4012513756752014),\n",
       " ('pictur', 0.39962583780288696)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"movi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finest', 0.753976583480835),\n",
       " ('funniest', 0.645499587059021),\n",
       " ('worst', 0.6364975571632385),\n",
       " ('weakest', 0.6331263184547424),\n",
       " ('greatest', 0.6280043125152588),\n",
       " ('coolest', 0.5717089176177979),\n",
       " ('scariest', 0.5357791185379028),\n",
       " ('saddest', 0.5317215323448181),\n",
       " ('poorest', 0.5277900695800781),\n",
       " ('strongest', 0.52679842710495)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model looks good so far. Each of these words has appropriate similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 14:52:27,457 : WARNING : direct access to syn0 will not be supported in future gensim releases, please use model.wv.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17218, 250)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7277.73\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7226.96\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7216.98\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7213.83\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7213.01\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7212.54\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7212.33\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7212.14\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7212.14\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7284.97\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7237.15\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7225.71\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7221.62\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7219.57\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7218.94\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7218.66\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7218.62\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7218.58\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7218.58\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7283.52\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7232.29\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7220.64\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7217.41\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7216.35\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7215.58\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7215.17\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7214.91\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7214.73\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7214.61\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 7214.59\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 7214.59\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7281.79\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7230.16\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7219.13\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7216.12\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7215.01\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7214.66\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7214.33\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7214.22\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7214.03\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7213.91\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 7213.91\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7284.17\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7234.17\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7222.62\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7219.39\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7217.98\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7217.11\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7216.73\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7216.5\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7216.46\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7216.46\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7280.24\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7232.0\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7221.57\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7217.44\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7216.04\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7215.62\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7215.49\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7215.42\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7215.42\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7285.69\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7232.55\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7222.04\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7218.51\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7217.09\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7216.67\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7216.38\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7216.08\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7215.87\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7215.79\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 7215.76\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 7215.76\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7284.27\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7232.21\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7221.33\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7218.77\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7217.31\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7217.1\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7217.07\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7217.07\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7289.9\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7237.28\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7226.29\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7223.11\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7222.2\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7221.99\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7221.85\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7221.7\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7221.55\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 7221.41\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 7221.41\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 7295.76\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 7241.41\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 7230.21\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 7227.69\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 7226.9\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 7226.64\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 7226.45\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 7226.41\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 7226.41\n",
      "center shift 0.000000e+00 within tolerance 3.604397e-07\n"
     ]
    }
   ],
   "source": [
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters,\n",
    "                           verbose = 2)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    \n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file //anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews are clean\n",
      "Testing reviews are clean\n"
     ]
    }
   ],
   "source": [
    "# Clean the training and testing reviews, remove stopwords.\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "print(\"Training reviews are clean\")  \n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "print(\"Testing reviews are clean\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'go',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 've',\n",
       " 'start',\n",
       " 'listen',\n",
       " 'music',\n",
       " ',',\n",
       " 'watch',\n",
       " 'odd',\n",
       " 'documentari',\n",
       " ',',\n",
       " 'watch',\n",
       " 'wiz',\n",
       " 'watch',\n",
       " 'moonwalk',\n",
       " 'mayb',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'realli',\n",
       " 'cool',\n",
       " 'eighti',\n",
       " 'mayb',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilti',\n",
       " 'innoc',\n",
       " 'moonwalk',\n",
       " 'part',\n",
       " 'biographi',\n",
       " ',',\n",
       " 'part',\n",
       " 'featur',\n",
       " 'film',\n",
       " 'rememb',\n",
       " 'go',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'origin',\n",
       " 'releas',\n",
       " 'subtl',\n",
       " 'messag',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'feel',\n",
       " 'toward',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'messag',\n",
       " 'drug',\n",
       " 'bad',\n",
       " \"m'kay\",\n",
       " 'visual',\n",
       " 'impress',\n",
       " 'cours',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remot',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'go',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'bore',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consent',\n",
       " 'make',\n",
       " 'movi',\n",
       " 'mj',\n",
       " 'fan',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fan',\n",
       " 'true',\n",
       " 'realli',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'featur',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'final',\n",
       " 'start',\n",
       " '20',\n",
       " 'minut',\n",
       " 'exclud',\n",
       " 'smooth',\n",
       " 'crimin',\n",
       " 'sequenc',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convinc',\n",
       " 'psychopath',\n",
       " 'power',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'want',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plan',\n",
       " '\\\\?',\n",
       " 'nah',\n",
       " ',',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " \"'s\",\n",
       " 'charact',\n",
       " 'rant',\n",
       " 'want',\n",
       " 'peopl',\n",
       " 'know',\n",
       " 'suppli',\n",
       " 'drug',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " ',',\n",
       " 'mayb',\n",
       " 'hate',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'music',\n",
       " 'lot',\n",
       " 'cool',\n",
       " 'thing',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turn',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequenc',\n",
       " 'also',\n",
       " ',',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patienc',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'film',\n",
       " 'kiddi',\n",
       " 'bad',\n",
       " 'sequenc',\n",
       " 'usual',\n",
       " 'director',\n",
       " 'hate',\n",
       " 'work',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alon',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'perform',\n",
       " 'complex',\n",
       " 'danc',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " ',',\n",
       " 'movi',\n",
       " 'peopl',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'anoth',\n",
       " '\\\\(',\n",
       " 'think',\n",
       " 'peopl',\n",
       " '\\\\)',\n",
       " ',',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'tri',\n",
       " 'give',\n",
       " 'wholesom',\n",
       " 'messag',\n",
       " 'iron',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'bestest',\n",
       " 'buddi',\n",
       " 'movi',\n",
       " 'girl',\n",
       " '!',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truli',\n",
       " 'one',\n",
       " 'talent',\n",
       " 'peopl',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilti',\n",
       " '\\\\?',\n",
       " 'well',\n",
       " ',',\n",
       " 'attent',\n",
       " 've',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'peopl',\n",
       " 'differ',\n",
       " 'behind',\n",
       " 'close',\n",
       " 'door',\n",
       " ',',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extrem',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liar',\n",
       " 'hope',\n",
       " 'latter']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'main',\n",
       " 'theme',\n",
       " 'mortal',\n",
       " ',',\n",
       " 'nostalgia',\n",
       " ',',\n",
       " 'loss',\n",
       " 'innoc',\n",
       " 'perhap',\n",
       " 'surpris',\n",
       " 'rate',\n",
       " 'high',\n",
       " 'older',\n",
       " 'viewer',\n",
       " 'younger',\n",
       " 'one',\n",
       " 'howev',\n",
       " 'craftsmanship',\n",
       " 'complet',\n",
       " 'film',\n",
       " 'anyon',\n",
       " 'enjoy',\n",
       " 'pace',\n",
       " 'steadi',\n",
       " 'constant',\n",
       " ',',\n",
       " 'charact',\n",
       " 'full',\n",
       " 'engag',\n",
       " ',',\n",
       " 'relationship',\n",
       " 'interact',\n",
       " 'natur',\n",
       " 'show',\n",
       " 'need',\n",
       " 'flood',\n",
       " 'tear',\n",
       " 'show',\n",
       " 'emot',\n",
       " ',',\n",
       " 'scream',\n",
       " 'show',\n",
       " 'fear',\n",
       " ',',\n",
       " 'shout',\n",
       " 'show',\n",
       " 'disput',\n",
       " 'violenc',\n",
       " 'show',\n",
       " 'anger',\n",
       " 'natur',\n",
       " 'joyc',\n",
       " \"'s\",\n",
       " 'short',\n",
       " 'stori',\n",
       " 'lend',\n",
       " 'film',\n",
       " 'readi',\n",
       " 'made',\n",
       " 'structur',\n",
       " 'perfect',\n",
       " 'polish',\n",
       " 'diamond',\n",
       " ',',\n",
       " 'small',\n",
       " 'chang',\n",
       " 'huston',\n",
       " 'make',\n",
       " 'inclus',\n",
       " 'poem',\n",
       " 'fit',\n",
       " 'neat',\n",
       " 'truli',\n",
       " 'masterpiec',\n",
       " 'tact',\n",
       " ',',\n",
       " 'subtleti',\n",
       " 'overwhelm',\n",
       " 'beauti']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews are complete.\n",
      "Testing reviews are complete.\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Training reviews are complete.\")    \n",
    "    \n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map )\n",
    "    counter += 1\n",
    "    \n",
    "print(\"Testing reviews are complete.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_centroids,\n",
    "                                                    train.sentiment,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   58.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'verbose': [True], 'n_estimators': [300], 'max_depth': [None], 'min_samples_leaf': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GridSearchCV to find the optimal parameters\n",
    "parameters = {'n_estimators':[100, 200, 300],\n",
    "              'max_depth':[1,3,5,7, None],\n",
    "              'min_samples_leaf': [1,3,5],\n",
    "              'verbose': [True]}\n",
    "\n",
    "# Use Random Forest to make the predictions\n",
    "forest = RandomForestClassifier()\n",
    "grid = GridSearchCV(forest, parameters)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training score =  0.85565\n",
      "Accuracy: 0.862400\n",
      "Best Parameters =  {'verbose': True, 'n_estimators': 300, 'max_depth': None, 'min_samples_leaf': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Best training score = \", grid.best_score_)\n",
    "\n",
    "grid_predictions = grid.best_estimator_.predict(x_test)\n",
    "grid_score = metrics.accuracy_score(y_test, grid_predictions) \n",
    "\n",
    "print(\"Accuracy: {0:f}\".format(grid_score))\n",
    "\n",
    "print(\"Best Parameters = \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pretty good first result here. The original code supplied by Google in the tutorial scored about 84%. It's nice that we have made some improvements to it, and scored higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 300\n",
      "building tree 3 of 300\n",
      "building tree 4 of 300\n",
      "building tree 5 of 300\n",
      "building tree 6 of 300\n",
      "building tree 7 of 300\n",
      "building tree 8 of 300\n",
      "building tree 9 of 300\n",
      "building tree 10 of 300\n",
      "building tree 11 of 300\n",
      "building tree 12 of 300\n",
      "building tree 13 of 300\n",
      "building tree 14 of 300\n",
      "building tree 15 of 300\n",
      "building tree 16 of 300\n",
      "building tree 17 of 300\n",
      "building tree 18 of 300\n",
      "building tree 19 of 300\n",
      "building tree 20 of 300\n",
      "building tree 21 of 300\n",
      "building tree 22 of 300\n",
      "building tree 23 of 300\n",
      "building tree 24 of 300\n",
      "building tree 25 of 300\n",
      "building tree 26 of 300\n",
      "building tree 27 of 300\n",
      "building tree 28 of 300\n",
      "building tree 29 of 300\n",
      "building tree 30 of 300\n",
      "building tree 31 of 300\n",
      "building tree 32 of 300\n",
      "building tree 33 of 300\n",
      "building tree 34 of 300\n",
      "building tree 35 of 300\n",
      "building tree 36 of 300\n",
      "building tree 37 of 300\n",
      "building tree 38 of 300\n",
      "building tree 39 of 300\n",
      "building tree 40 of 300\n",
      "building tree 41 of 300\n",
      "building tree 42 of 300\n",
      "building tree 43 of 300\n",
      "building tree 44 of 300\n",
      "building tree 45 of 300\n",
      "building tree 46 of 300\n",
      "building tree 47 of 300\n",
      "building tree 48 of 300\n",
      "building tree 49 of 300\n",
      "building tree 50 of 300\n",
      "building tree 51 of 300\n",
      "building tree 52 of 300\n",
      "building tree 53 of 300\n",
      "building tree 54 of 300\n",
      "building tree 55 of 300\n",
      "building tree 56 of 300\n",
      "building tree 57 of 300\n",
      "building tree 58 of 300\n",
      "building tree 59 of 300\n",
      "building tree 60 of 300\n",
      "building tree 61 of 300\n",
      "building tree 62 of 300\n",
      "building tree 63 of 300\n",
      "building tree 64 of 300\n",
      "building tree 65 of 300\n",
      "building tree 66 of 300\n",
      "building tree 67 of 300\n",
      "building tree 68 of 300\n",
      "building tree 69 of 300\n",
      "building tree 70 of 300\n",
      "building tree 71 of 300\n",
      "building tree 72 of 300\n",
      "building tree 73 of 300\n",
      "building tree 74 of 300\n",
      "building tree 75 of 300\n",
      "building tree 76 of 300\n",
      "building tree 77 of 300\n",
      "building tree 78 of 300\n",
      "building tree 79 of 300\n",
      "building tree 80 of 300\n",
      "building tree 81 of 300\n",
      "building tree 82 of 300\n",
      "building tree 83 of 300\n",
      "building tree 84 of 300\n",
      "building tree 85 of 300\n",
      "building tree 86 of 300\n",
      "building tree 87 of 300\n",
      "building tree 88 of 300\n",
      "building tree 89 of 300\n",
      "building tree 90 of 300\n",
      "building tree 91 of 300\n",
      "building tree 92 of 300\n",
      "building tree 93 of 300\n",
      "building tree 94 of 300\n",
      "building tree 95 of 300\n",
      "building tree 96 of 300\n",
      "building tree 97 of 300\n",
      "building tree 98 of 300\n",
      "building tree 99 of 300\n",
      "building tree 100 of 300\n",
      "building tree 101 of 300\n",
      "building tree 102 of 300\n",
      "building tree 103 of 300\n",
      "building tree 104 of 300\n",
      "building tree 105 of 300\n",
      "building tree 106 of 300\n",
      "building tree 107 of 300\n",
      "building tree 108 of 300\n",
      "building tree 109 of 300\n",
      "building tree 110 of 300\n",
      "building tree 111 of 300\n",
      "building tree 112 of 300\n",
      "building tree 113 of 300\n",
      "building tree 114 of 300\n",
      "building tree 115 of 300\n",
      "building tree 116 of 300\n",
      "building tree 117 of 300\n",
      "building tree 118 of 300\n",
      "building tree 119 of 300\n",
      "building tree 120 of 300\n",
      "building tree 121 of 300\n",
      "building tree 122 of 300\n",
      "building tree 123 of 300\n",
      "building tree 124 of 300\n",
      "building tree 125 of 300\n",
      "building tree 126 of 300\n",
      "building tree 127 of 300\n",
      "building tree 128 of 300\n",
      "building tree 129 of 300\n",
      "building tree 130 of 300\n",
      "building tree 131 of 300\n",
      "building tree 132 of 300\n",
      "building tree 133 of 300\n",
      "building tree 134 of 300\n",
      "building tree 135 of 300\n",
      "building tree 136 of 300\n",
      "building tree 137 of 300\n",
      "building tree 138 of 300\n",
      "building tree 139 of 300\n",
      "building tree 140 of 300\n",
      "building tree 141 of 300\n",
      "building tree 142 of 300\n",
      "building tree 143 of 300\n",
      "building tree 144 of 300\n",
      "building tree 145 of 300\n",
      "building tree 146 of 300\n",
      "building tree 147 of 300\n",
      "building tree 148 of 300\n",
      "building tree 149 of 300\n",
      "building tree 150 of 300\n",
      "building tree 151 of 300\n",
      "building tree 152 of 300\n",
      "building tree 153 of 300\n",
      "building tree 154 of 300\n",
      "building tree 155 of 300\n",
      "building tree 156 of 300\n",
      "building tree 157 of 300\n",
      "building tree 158 of 300\n",
      "building tree 159 of 300\n",
      "building tree 160 of 300\n",
      "building tree 161 of 300\n",
      "building tree 162 of 300\n",
      "building tree 163 of 300\n",
      "building tree 164 of 300\n",
      "building tree 165 of 300\n",
      "building tree 166 of 300\n",
      "building tree 167 of 300\n",
      "building tree 168 of 300\n",
      "building tree 169 of 300\n",
      "building tree 170 of 300\n",
      "building tree 171 of 300\n",
      "building tree 172 of 300\n",
      "building tree 173 of 300\n",
      "building tree 174 of 300\n",
      "building tree 175 of 300\n",
      "building tree 176 of 300\n",
      "building tree 177 of 300\n",
      "building tree 178 of 300\n",
      "building tree 179 of 300\n",
      "building tree 180 of 300\n",
      "building tree 181 of 300\n",
      "building tree 182 of 300\n",
      "building tree 183 of 300\n",
      "building tree 184 of 300\n",
      "building tree 185 of 300\n",
      "building tree 186 of 300\n",
      "building tree 187 of 300\n",
      "building tree 188 of 300\n",
      "building tree 189 of 300\n",
      "building tree 190 of 300\n",
      "building tree 191 of 300\n",
      "building tree 192 of 300\n",
      "building tree 193 of 300\n",
      "building tree 194 of 300\n",
      "building tree 195 of 300\n",
      "building tree 196 of 300\n",
      "building tree 197 of 300\n",
      "building tree 198 of 300\n",
      "building tree 199 of 300\n",
      "building tree 200 of 300\n",
      "building tree 201 of 300\n",
      "building tree 202 of 300\n",
      "building tree 203 of 300\n",
      "building tree 204 of 300\n",
      "building tree 205 of 300\n",
      "building tree 206 of 300\n",
      "building tree 207 of 300\n",
      "building tree 208 of 300\n",
      "building tree 209 of 300\n",
      "building tree 210 of 300\n",
      "building tree 211 of 300\n",
      "building tree 212 of 300\n",
      "building tree 213 of 300\n",
      "building tree 214 of 300\n",
      "building tree 215 of 300\n",
      "building tree 216 of 300\n",
      "building tree 217 of 300\n",
      "building tree 218 of 300\n",
      "building tree 219 of 300\n",
      "building tree 220 of 300\n",
      "building tree 221 of 300\n",
      "building tree 222 of 300\n",
      "building tree 223 of 300\n",
      "building tree 224 of 300\n",
      "building tree 225 of 300\n",
      "building tree 226 of 300\n",
      "building tree 227 of 300\n",
      "building tree 228 of 300\n",
      "building tree 229 of 300\n",
      "building tree 230 of 300\n",
      "building tree 231 of 300\n",
      "building tree 232 of 300\n",
      "building tree 233 of 300\n",
      "building tree 234 of 300\n",
      "building tree 235 of 300\n",
      "building tree 236 of 300\n",
      "building tree 237 of 300\n",
      "building tree 238 of 300\n",
      "building tree 239 of 300\n",
      "building tree 240 of 300\n",
      "building tree 241 of 300\n",
      "building tree 242 of 300\n",
      "building tree 243 of 300\n",
      "building tree 244 of 300\n",
      "building tree 245 of 300\n",
      "building tree 246 of 300\n",
      "building tree 247 of 300\n",
      "building tree 248 of 300\n",
      "building tree 249 of 300\n",
      "building tree 250 of 300\n",
      "building tree 251 of 300\n",
      "building tree 252 of 300\n",
      "building tree 253 of 300\n",
      "building tree 254 of 300\n",
      "building tree 255 of 300\n",
      "building tree 256 of 300\n",
      "building tree 257 of 300\n",
      "building tree 258 of 300\n",
      "building tree 259 of 300\n",
      "building tree 260 of 300\n",
      "building tree 261 of 300\n",
      "building tree 262 of 300\n",
      "building tree 263 of 300\n",
      "building tree 264 of 300\n",
      "building tree 265 of 300\n",
      "building tree 266 of 300\n",
      "building tree 267 of 300\n",
      "building tree 268 of 300\n",
      "building tree 269 of 300\n",
      "building tree 270 of 300\n",
      "building tree 271 of 300\n",
      "building tree 272 of 300\n",
      "building tree 273 of 300\n",
      "building tree 274 of 300\n",
      "building tree 275 of 300\n",
      "building tree 276 of 300\n",
      "building tree 277 of 300\n",
      "building tree 278 of 300\n",
      "building tree 279 of 300\n",
      "building tree 280 of 300\n",
      "building tree 281 of 300\n",
      "building tree 282 of 300\n",
      "building tree 283 of 300\n",
      "building tree 284 of 300\n",
      "building tree 285 of 300\n",
      "building tree 286 of 300\n",
      "building tree 287 of 300\n",
      "building tree 288 of 300\n",
      "building tree 289 of 300\n",
      "building tree 290 of 300\n",
      "building tree 291 of 300\n",
      "building tree 292 of 300\n",
      "building tree 293 of 300\n",
      "building tree 294 of 300\n",
      "building tree 295 of 300\n",
      "building tree 296 of 300\n",
      "building tree 297 of 300\n",
      "building tree 298 of 300\n",
      "building tree 299 of 300\n",
      "building tree 300 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.6s finished\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 300,\n",
    "                                max_depth = None,\n",
    "                                min_samples_leaf = 1, \n",
    "                                verbose = 2)\n",
    "\n",
    "# Apply the Random Forest Model to the full training data.\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I submit the results to the Kaggle competition its accuracy is 85.7%, which ranks it near the middle of the pack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model #2: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the length of each training and testing review\n",
    "review_lengths = []\n",
    "for review in clean_train_reviews:\n",
    "    review_lengths.append(len(review))\n",
    "    \n",
    "review_lengths_test = []\n",
    "for review in clean_test_reviews:\n",
    "    review_lengths_test.append(len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the lists to dataframes so that describe() can be used\n",
    "review_lengths = pd.DataFrame(review_lengths)\n",
    "review_lengths_test = pd.DataFrame(review_lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Training Reviews:\n",
      "                  0\n",
      "count  25000.000000\n",
      "mean     141.511440\n",
      "std      107.887134\n",
      "min        4.000000\n",
      "25%       74.000000\n",
      "50%      105.000000\n",
      "75%      173.000000\n",
      "max     1571.000000\n",
      "\n",
      "Summary Testing Reviews:\n",
      "                  0\n",
      "count  25000.000000\n",
      "mean     138.179320\n",
      "std      105.029206\n",
      "min        6.000000\n",
      "25%       74.000000\n",
      "50%      103.000000\n",
      "75%      168.000000\n",
      "max     1464.000000\n"
     ]
    }
   ],
   "source": [
    "# Print out a summary of the review lengths\n",
    "print(\"Summary Training Reviews:\")\n",
    "print(review_lengths.describe())\n",
    "print()\n",
    "print(\"Summary Testing Reviews:\")\n",
    "print(review_lengths_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281.0\n",
      "272.0\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum number of words for a percentile\n",
    "percentile = 90\n",
    "print(np.percentile(review_lengths, percentile))\n",
    "print(np.percentile(review_lengths_test, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the list of words to make more natural sentences\n",
    "clean_train_reviews_sentences = []\n",
    "space = \" \"\n",
    "for review in clean_train_reviews:\n",
    "    sentence = space.join(review)\n",
    "    clean_train_reviews_sentences.append(sentence)\n",
    "    \n",
    "clean_test_reviews_sentences = []\n",
    "space = \" \"\n",
    "for review in clean_test_reviews:\n",
    "    sentence = space.join(review)\n",
    "    clean_test_reviews_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"stuff go moment mj ve start listen music , watch odd documentari , watch wiz watch moonwalk mayb want get certain insight guy thought realli cool eighti mayb make mind whether guilti innoc moonwalk part biographi , part featur film rememb go see cinema origin releas subtl messag mj 's feel toward press also obvious messag drug bad m'kay visual impress cours michael jackson unless remot like mj anyway go hate find bore may call mj egotist consent make movi mj fan would say made fan true realli nice actual featur film bit final start 20 minut exclud smooth crimin sequenc joe pesci convinc psychopath power drug lord want mj dead bad beyond mj overheard plan \\\\? nah , joe pesci 's charact rant want peopl know suppli drug etc dunno , mayb hate mj 's music lot cool thing like mj turn car robot whole speed demon sequenc also , director must patienc saint came film kiddi bad sequenc usual director hate work one kid let alon whole bunch perform complex danc scene bottom line , movi peopl like mj one level anoth \\\\( think peopl \\\\) , stay away tri give wholesom messag iron mj 's bestest buddi movi girl ! michael jackson truli one talent peopl ever grace planet guilti \\\\? well , attent ve gave subject hmmm well n't know peopl differ behind close door , know fact either extrem nice stupid guy one sickest liar hope latter\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at a review to ensure everything is alright\n",
    "clean_train_reviews_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data in training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_train_reviews_sentences, \n",
    "                                                    train.sentiment, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 16625\n"
     ]
    }
   ],
   "source": [
    "# Process the reviews to limit the number of words and length\n",
    "\n",
    "# max_document_length = maximum number of words in a review\n",
    "# min_frequency = minimum number of times a word must be present to be used in the vocabulary\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 281,\n",
    "                                                          min_frequency = 5)\n",
    "x_train_transformed = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "x_test_transformed = np.array(list(vocab_processor.transform(x_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  316,   845, 10973,   263,   597,   233,   694,    79,  2270,\n",
       "         350,  1708,   494,  1822,    79,     5,    79,  6216,   854,\n",
       "           0,   674,     5,  1369,     0,    34,    14,  1979,   221,\n",
       "           6,   502,     3,   310,  1336,     0,   605, 16190,    79,\n",
       "       10973,   263, 10973,     1,    55,    10,   502,     3,     6,\n",
       "         494,  1822,    79,  2270,   350,  2580,   929,   793,   177,\n",
       "        8210,     3, 16190,     1,  1331,  1290,    77,   356,     5,\n",
       "         124,    83,    95,   210,     3,  9115, 10973,    10,   591,\n",
       "         189,    82,  3264,   120,   177,    90,    32,    35,   558,\n",
       "           5,   124,   214,   241,     6,    73,     2,    79,    14,\n",
       "         175,  7112,    49,   248,  9367,    70,  2782,     0,  1843,\n",
       "        7131,  1931,    34,   624,  6359,   711,    63,  8158,     3,\n",
       "         721,   189, 10973,     1,   223,    12,    86,   263,   597,\n",
       "        1263,   578,    63,   114, 16190, 10973,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure everything looks okay\n",
    "x_train_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particular joe mcdoak short subject obvious inspir star warner brother spectacular thank lucki star , one star wartim moral booster period one eddi cantor play would comedian 'd like break film except resembl cantor georg o'hanlon star mcdoak short mcdoak 's tri get break film like thank lucki star warner brother contract player free moment stroll film o'hanlon 's sent central cast small one line role world war film , lookalik mcdoak get messag poor guy nervous big moment , start think way deliv one line mayb sound like real movi star would help 86 take later exasper director ralph sanford patient clyde cook play british cockney soldier find nich film busi poor mcdoak 's worth see funni short subject nomin oscar find happen o'hanlon mcdoak\n",
      "\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print()\n",
    "print(len(x_train[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 15\n",
    "\n",
    "def bag_of_words_model(features, target):    \n",
    "    # One-hot encode the target feature - positive and negative\n",
    "    target = tf.one_hot(target, 2, 1, 0)  \n",
    "    \n",
    "    # If you alter the original n_words, you will need to input the value manually.\n",
    "    features = tf.contrib.layers.bow_encoder(features, \n",
    "                                             vocab_size = 18307, #n_words, \n",
    "                                             embed_dim = EMBEDDING_SIZE)  \n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(features, \n",
    "                                               2, \n",
    "                                               activation_fn=None)  \n",
    "    \n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    \n",
    "    train_op = tf.contrib.layers.optimize_loss(loss, \n",
    "                                               tf.contrib.framework.get_global_step(),\n",
    "                                               optimizer='Adam', \n",
    "                                               learning_rate=0.005)  \n",
    "    \n",
    "    return ({'class': tf.argmax(logits, 1), \n",
    "             'prob': tf.nn.softmax(logits)},      \n",
    "            loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:33,237 : WARNING : Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:33,252 : INFO : Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1f8d6fd30>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:33,254 : INFO : Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1f8d6fd30>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-ef0b5db21691>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:33,504 : WARNING : From <ipython-input-46-ef0b5db21691>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-ef0b5db21691>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:33,523 : WARNING : From <ipython-input-46-ef0b5db21691>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:37,729 : INFO : Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.693168, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,052 : INFO : loss = 0.693168, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,053 : INFO : Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,055 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,056 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,058 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,060 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,061 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:23:41,063 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.254101, step = 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:25:13,504 : INFO : loss = 0.254101, step = 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.08164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:25:13,506 : INFO : global_step/sec: 1.08164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.116709, step = 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:26:39,230 : INFO : loss = 0.116709, step = 201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.16651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:26:39,232 : INFO : global_step/sec: 1.16651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0620678, step = 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:28:05,628 : INFO : loss = 0.0620678, step = 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.15744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:28:05,629 : INFO : global_step/sec: 1.15744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0354963, step = 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:29:31,003 : INFO : loss = 0.0354963, step = 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.17131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:29:31,004 : INFO : global_step/sec: 1.17131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0217942, step = 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:30:56,350 : INFO : loss = 0.0217942, step = 501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.17169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:30:56,351 : INFO : global_step/sec: 1.17169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0143387, step = 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:32:21,959 : INFO : loss = 0.0143387, step = 601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.16809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:32:21,961 : INFO : global_step/sec: 1.16809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 694 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,346 : INFO : Saving checkpoints for 694 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,347 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,348 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,349 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,350 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,352 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:41,353 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0100055, step = 701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:47,924 : INFO : loss = 0.0100055, step = 701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.16327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:33:47,925 : INFO : global_step/sec: 1.16327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.00731796, step = 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:35:25,411 : INFO : loss = 0.00731796, step = 801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1.02577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:35:25,413 : INFO : global_step/sec: 1.02577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.00555508, step = 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:37:18,420 : INFO : loss = 0.00555508, step = 901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.884885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:37:18,422 : INFO : global_step/sec: 0.884885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,190 : INFO : Saving checkpoints for 1000 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,191 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,192 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,195 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,198 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,199 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,200 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.00435374.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:07,852 : INFO : Loss for final step: 0.00435374.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set classifier as bag_of_words_model\n",
    "classifier_bow = learn.Estimator(model_fn = bag_of_words_model) \n",
    "# Train model\n",
    "classifier_bow.fit(x_train_transformed, y_train, steps=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-47-f186ae4fee03>:1 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:08,213 : WARNING : From <ipython-input-47-f186ae4fee03>:1 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-47-f186ae4fee03>:1 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:08,232 : WARNING : From <ipython-input-47-f186ae4fee03>:1 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt-1000-?????-of-00001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:09,088 : INFO : Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpncljsidi/model.ckpt-1000-?????-of-00001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.850600\n"
     ]
    }
   ],
   "source": [
    "bow_predictions = [p['class'] for p in classifier_bow.predict(x_test_transformed, as_iterable=True)] \n",
    "score = metrics.accuracy_score(y_test, bow_predictions) \n",
    "print(\"Accuracy: {0:f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of 85.06% is pretty similar to the Bag of Centroids model. Let's see how it compares when we use all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 18307\n"
     ]
    }
   ],
   "source": [
    "# Process the full data set to make final predictions\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 281,\n",
    "                                                          min_frequency = 5)\n",
    "x_train_all = np.array(list(vocab_processor.fit_transform(clean_train_reviews_sentences)))\n",
    "x_test_all = np.array(list(vocab_processor.transform(clean_test_reviews_sentences)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:26,449 : WARNING : Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:26,462 : INFO : Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x21e306588>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:26,464 : INFO : Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x21e306588>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-0e4b9345a3d2>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:26,526 : WARNING : From <ipython-input-49-0e4b9345a3d2>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-0e4b9345a3d2>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:26,551 : WARNING : From <ipython-input-49-0e4b9345a3d2>:4 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:28,001 : INFO : Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.693106, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,256 : INFO : loss = 0.693106, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,258 : INFO : Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,259 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,260 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,261 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,262 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,263 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:39:36,264 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.261839, step = 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:42:07,473 : INFO : loss = 0.261839, step = 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.661304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:42:07,474 : INFO : global_step/sec: 0.661304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.127082, step = 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:44:38,785 : INFO : loss = 0.127082, step = 201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.660866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:44:38,792 : INFO : global_step/sec: 0.660866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0718937, step = 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:46:59,225 : INFO : loss = 0.0718937, step = 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.712068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:46:59,227 : INFO : global_step/sec: 0.712068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.043267, step = 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:31,387 : INFO : loss = 0.043267, step = 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.657176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:31,395 : INFO : global_step/sec: 0.657176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 405 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,388 : INFO : Saving checkpoints for 405 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,389 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,391 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,392 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,393 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,395 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:49:37,397 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0273597, step = 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:52:14,244 : INFO : loss = 0.0273597, step = 501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.614051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:52:14,247 : INFO : global_step/sec: 0.614051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.018212, step = 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:54:55,048 : INFO : loss = 0.018212, step = 601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.621875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:54:55,051 : INFO : global_step/sec: 0.621875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0127388, step = 701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:57:23,882 : INFO : loss = 0.0127388, step = 701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.671893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:57:23,884 : INFO : global_step/sec: 0.671893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 790 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,657 : INFO : Saving checkpoints for 790 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,658 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,660 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,661 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,663 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,664 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 15:59:37,666 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.00930042, step = 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:00:01,916 : INFO : loss = 0.00930042, step = 801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.632776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:00:01,918 : INFO : global_step/sec: 0.632776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.00703519, step = 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:02:53,548 : INFO : loss = 0.00703519, step = 901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.582623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:02:53,557 : INFO : global_step/sec: 0.582623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,220 : INFO : Saving checkpoints for 1000 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,222 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,223 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,225 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,226 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,228 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:28,231 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.00549149.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:29,604 : INFO : Loss for final step: 0.00549149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-0e4b9345a3d2>:6 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:29,946 : WARNING : From <ipython-input-49-0e4b9345a3d2>:6 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-0e4b9345a3d2>:6 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:29,965 : WARNING : From <ipython-input-49-0e4b9345a3d2>:6 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt-1000-?????-of-00001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 16:05:30,840 : INFO : Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpheckb5hr/model.ckpt-1000-?????-of-00001.\n"
     ]
    }
   ],
   "source": [
    "# Need to use learn.Estimator again to 'reset' the model. \n",
    "# Otherwise you would be 'double training.'\n",
    "classifier_bow = learn.Estimator(model_fn = bag_of_words_model) \n",
    "classifier_bow.fit(x_train_all, train.sentiment, steps=1000) \n",
    "\n",
    "result_bow = [p['class'] for p in classifier_bow.predict(x_test_all, as_iterable=True)] \n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result_bow})\n",
    "output.to_csv(\"BagOfWords.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy drops when we use the Kaggle predictions to 83.7%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model #3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 25\n",
    "LTSM_SIZE = 25\n",
    "number_of_layers = 3\n",
    "\n",
    "def rnn_model(features, target):  \n",
    "    \"\"\"RNN model to predict from sequence of words to a class.\"\"\"  \n",
    "    \n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE]\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(features, \n",
    "                                                    vocab_size = 9600, #n_words, \n",
    "                                                    embed_dim = EMBEDDING_SIZE)   \n",
    "    \n",
    "    # Split into list of embeddings per word, while removing doc length dim.\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "    \n",
    "    # Create a Long Short Term Memory cell with hidden size of LISTM_SIZE.\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(LTSM_SIZE, state_is_tuple=False)\n",
    "    \n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # max_document_length and passes word_list as inputs for each unit.\n",
    "    _, encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)   \n",
    "\n",
    "    target = tf.one_hot(target, 2, 1, 0)\n",
    "    logits = tf.contrib.layers.fully_connected(encoding, 2, activation_fn=None)  \n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)   \n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss, \n",
    "                                               tf.contrib.framework.get_global_step(),      \n",
    "                                               optimizer='Adam', \n",
    "                                               learning_rate=0.005, \n",
    "                                               clip_gradients=1.0)   \n",
    "    return ({'class': tf.argmax(logits, 1), \n",
    "             'prob': tf.nn.softmax(logits)},      \n",
    "             loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 8482\n"
     ]
    }
   ],
   "source": [
    "# Need to process the reviews again, because my laptop will crash if the network is too large.\n",
    "# If you are using a GPU or have more than 8GB of RAM, you should be able to use more data for training.\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 150,\n",
    "                                                          min_frequency = 20)\n",
    "x_train_transformed = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "x_test_transformed = np.array(list(vocab_processor.transform(x_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-668cc603eb84>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:36:54,793 : WARNING : From <ipython-input-57-668cc603eb84>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-57-668cc603eb84>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:36:54,815 : WARNING : From <ipython-input-57-668cc603eb84>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x20d7124e0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:36:54,924 : WARNING : <tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x20d7124e0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpvggc98tl/model.ckpt-500-?????-of-00001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:37:31,277 : INFO : Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpvggc98tl/model.ckpt-500-?????-of-00001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.832200\n"
     ]
    }
   ],
   "source": [
    "classifier_rnn = learn.Estimator(model_fn = rnn_model) \n",
    "classifier_rnn.fit(x_train_transformed, y_train, steps = 500) \n",
    "\n",
    "predictions_rnn = [p['class'] for p in classifier_rnn.predict(x_test_transformed, as_iterable=True)] \n",
    "score = metrics.accuracy_score(y_test, predictions_rnn) \n",
    "print(\"Accuracy: {0:f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy for the LSTM model is the lowest, at 83.22%. I expect this is due to the smaller amount of data that is being used to train this neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 9600\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 150,\n",
    "                                                          min_frequency = 20)\n",
    "x_train_all_rnn = np.array(list(vocab_processor.fit_transform(clean_train_reviews_sentences)))\n",
    "x_test_all_rnn = np.array(list(vocab_processor.transform(clean_test_reviews_sentences)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,734 : WARNING : Using temporary folder as model directory: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,737 : INFO : Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x187de77b8>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,739 : INFO : Using config: {'save_checkpoints_steps': None, 'save_checkpoints_secs': 600, '_is_chief': True, '_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x187de77b8>, '_task_id': 0, '_environment': 'local', 'keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_evaluation_master': '', 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-61-ee7529e2b99d>:2 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,756 : WARNING : From <ipython-input-61-ee7529e2b99d>:2 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-61-ee7529e2b99d>:2 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,774 : WARNING : From <ipython-input-61-ee7529e2b99d>:2 in <module>.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x20d4d0400>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:39:50,827 : WARNING : <tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x20d4d0400>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:40:53,643 : INFO : Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.693123, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,591 : INFO : loss = 0.693123, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,601 : INFO : Saving checkpoints for 1 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,605 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,606 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,608 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,609 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,610 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:42:23,611 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 43 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,953 : INFO : Saving checkpoints for 43 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,959 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,960 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,961 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,962 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,963 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 18:52:35,965 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 81 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,174 : INFO : Saving checkpoints for 81 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,182 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,184 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,184 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,185 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,187 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:02:52,188 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.30135, step = 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:08:11,739 : INFO : loss = 0.30135, step = 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0645934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:08:11,769 : INFO : global_step/sec: 0.0645934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 122 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,282 : INFO : Saving checkpoints for 122 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,284 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,285 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,286 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,287 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,289 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:13:05,290 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 162 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,112 : INFO : Saving checkpoints for 162 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,118 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,119 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,120 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,121 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,147 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:23:17,148 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.00452801, step = 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:32:46,876 : INFO : loss = 0.00452801, step = 201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0677903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:32:46,883 : INFO : global_step/sec: 0.0677903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 204 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,787 : INFO : Saving checkpoints for 204 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,788 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,789 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,791 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,792 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,793 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:33:28,794 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 246 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,309 : INFO : Saving checkpoints for 246 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,316 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,317 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,318 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,319 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,320 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:43:41,321 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 288 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,378 : INFO : Saving checkpoints for 288 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,384 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,385 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,386 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,387 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,388 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:53:53,389 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.000141494, step = 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:57:23,034 : INFO : loss = 0.000141494, step = 301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0677434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 19:57:23,041 : INFO : global_step/sec: 0.0677434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 327 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,950 : INFO : Saving checkpoints for 327 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,956 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,957 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,958 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,959 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,960 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:04:01,961 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 356 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,240 : INFO : Saving checkpoints for 356 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,246 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,352 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,459 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,490 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,491 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:14:16,493 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 386 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,537 : INFO : Saving checkpoints for 386 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,569 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,570 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,571 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,573 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,574 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:24:22,639 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 3.63579e-05, step = 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:29:20,676 : INFO : loss = 3.63579e-05, step = 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0521474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:29:20,684 : INFO : global_step/sec: 0.0521474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 417 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,717 : INFO : Saving checkpoints for 417 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,726 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,728 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,729 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,731 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,733 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:34:37,734 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 454 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,695 : INFO : Saving checkpoints for 454 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,702 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,702 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,703 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,704 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,705 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:44:45,706 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 497 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,834 : INFO : Saving checkpoints for 497 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,840 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,841 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,842 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,843 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,844 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:54:49,845 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,034 : INFO : Saving checkpoints for 500 into /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,036 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,037 : WARNING : TensorFlow's V1 checkpoint format has been deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,039 : WARNING : Consider switching to the more efficient V2 format:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,040 : WARNING :    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:now on by default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,041 : WARNING : now on by default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:55:51,042 : WARNING : *******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 2.13439e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:56:10,961 : INFO : Loss for final step: 2.13439e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-61-ee7529e2b99d>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:56:11,796 : WARNING : From <ipython-input-61-ee7529e2b99d>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-61-ee7529e2b99d>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:56:11,813 : WARNING : From <ipython-input-61-ee7529e2b99d>:4 in <module>.: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x2244c6e80>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:56:12,285 : WARNING : <tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x2244c6e80>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt-500-?????-of-00001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-24 20:56:42,182 : INFO : Loading model from checkpoint: /var/folders/h3/j2h_850j5klb8yns26kmxqfw0000gp/T/tmpn02idq1e/model.ckpt-500-?????-of-00001.\n"
     ]
    }
   ],
   "source": [
    "classifier_rnn = learn.Estimator(model_fn = rnn_model) \n",
    "classifier_rnn.fit(x_train_all_rnn, train.sentiment, steps=500) \n",
    "\n",
    "result_rnn = [p['class'] for p in classifier_rnn.predict(x_test_all_rnn, as_iterable=True)] \n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result_rnn})\n",
    "output.to_csv(\"rnn_predictions.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the testing data, it was not too surprising to see that the LSTM model also scored the worst on the submission data, 81.3%. It would be interesting to see how much the score would improve if we were able to use more data in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
